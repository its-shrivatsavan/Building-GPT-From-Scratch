Building GPT From Scratch

Project Overview

This project focuses on developing a Generative Pre-trained Transformer (GPT) model from scratch, inspired by the "Attention is All You Need" paper. The primary goal is to implement key components of the GPT architecture, such as positional encoding, and multi-head self-attention, to build a foundational understanding of how these models work under the hood.

Features

  Tokenization: Conversion of raw text into tokens that the model can process.
  
  Positional Encoding: Incorporation of positional information into the tokenized input, allowing the model to understand the order of words.
  
  Multi-Head Self-Attention: Implementation of self-attention mechanisms to capture dependencies between different words in a sentence.
  
  Transformer Blocks: Construction of multiple transformer layers to build the full GPT model.

Files in the Repository

BuildingGPT.ipynb: A Jupyter notebook containing the code to build the GPT model from scratch, step by step.
